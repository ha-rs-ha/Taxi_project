{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+bo7RX7SyYweIvU6iyD5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ha-rs-ha/Taxi_project/blob/main/taxi_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UgR6nP7WK5i4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "state = env.reset()\n",
        "env.render()\n",
        "print(\"Current state is  :\", state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHszBQS8K9hV",
        "outputId": "10b9ce47-5c1f-403c-b69d-7349a172022d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state is  : 284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5*5*4*5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm6cfbpZLAFO",
        "outputId": "45ffb024-4ce2-41ab-e4a9-f241cee69cb9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = env.observation_space.n\n",
        "print(\"State Space :\", state_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6agjlutFLr3W",
        "outputId": "4bcac2ec-ef6c-42c9-c72d-6f934343fdaf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Space : 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_size = env.action_space.n\n",
        "print(\"Action Space :\", action_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3tJl2YYLueK",
        "outputId": "1dc250de-d204-40cd-9b57-97eb58d8e9a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithms:\n",
        "\n",
        "\n",
        "*   Q Learning\n",
        "*    SARSA\n",
        "\n"
      ],
      "metadata": {
        "id": "7sTfYBTDL0GV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Q Learning"
      ],
      "metadata": {
        "id": "M_iEDoH0MCsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros((state_size, action_size))\n",
        "episodes = 100000\n",
        "learning_rate = 0.1\n",
        "gamma = 0.7\n",
        "epsilon = 0.1\n",
        "\n",
        "def greedy_policy(state, table):\n",
        "    z = np.random.random()\n",
        "    if z > epsilon:\n",
        "        action = np.argmax(table[state])\n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "    return action"
      ],
      "metadata": {
        "id": "t5T6GYE2Lw63"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deltas = []\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    step = 0\n",
        "    change_t = 0\n",
        "\n",
        "    if episode % 5000 == 0:\n",
        "        print(\"Episode: {}\".format(episode))\n",
        "    while not done:\n",
        "        #env.render()\n",
        "        action = greedy_policy(state, q_table)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        old_q = q_table[state, action]\n",
        "\n",
        "        #Update\n",
        "        q_table[state, action] += learning_rate * (reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])\n",
        "        change_t = max(change_t, np.abs(q_table[state][action] - old_q))\n",
        "        state = new_state\n",
        "    deltas.append(change_t)\n",
        "    if deltas[-1] < 0.000000001:\n",
        "        break\n",
        "    episode += 1\n",
        "print(\"Maximum Difference is :\", deltas[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHzQJs03MFq3",
        "outputId": "84146ee0-975c-4d87-acc2-dcb3b7e9e254"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Difference is : 9.509124598849894e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "VsbEgLS1MIUR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "metadata": {
        "id": "2qdeACshMOZl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while(done==False):\n",
        "    best_action = np.argmax(q_table[state, :])\n",
        "\n",
        "    state, reward, done, _ = env.step(best_action)\n",
        "\n",
        "    cumulative_reward += reward\n",
        "    time.sleep(0.5)\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    print('Episode Reward :', cumulative_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_Q5qcytMQVj",
        "outputId": "7be370f3-418c-4216-a43a-41c196caf7ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode Reward : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. SARSA"
      ],
      "metadata": {
        "id": "nF4H-XiBMWFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "hNJlsB18MTCU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_table = np.zeros((state_size, action_size))\n",
        "episodes = 100000\n",
        "deltas = []\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done =False\n",
        "    step = 0\n",
        "    change_t = 0\n",
        "    if episode % 5000 == 0:\n",
        "        print(\"Episode: {}\".format(episode))\n",
        "\n",
        "    while not done:\n",
        "        action = greedy_policy(state, s_table)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_action = greedy_policy(state, s_table)\n",
        "        old_t = s_table[state, action]\n",
        "\n",
        "        s_table[state, action] += learning_rate * (reward+gamma* s_table[new_state, next_action] - s_table[state, action])\n",
        "        change_t = max(change_t, np.abs(s_table[state][action] - old_t))\n",
        "\n",
        "        state = new_state\n",
        "    deltas.append(change_t)\n",
        "    if deltas[-1] < 0.000000001:\n",
        "        break\n",
        "    episode += 1\n",
        "print(\"Maximum Difference is :\", deltas[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRD1NbG4MYUa",
        "outputId": "685b4725-52d6-46f8-b39f-f9fb85a7a78b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 5000\n",
            "Episode: 10000\n",
            "Episode: 15000\n",
            "Episode: 20000\n",
            "Episode: 25000\n",
            "Episode: 30000\n",
            "Episode: 35000\n",
            "Episode: 40000\n",
            "Episode: 45000\n",
            "Episode: 50000\n",
            "Episode: 55000\n",
            "Episode: 60000\n",
            "Episode: 65000\n",
            "Episode: 70000\n",
            "Episode: 75000\n",
            "Episode: 80000\n",
            "Episode: 85000\n",
            "Episode: 90000\n",
            "Episode: 95000\n",
            "Episode: 100000\n",
            "Maximum Difference is : 0.591939608498707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "8zuQXSZlMdHi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while(done==False):\n",
        "    best_action = np.argmax(s_table[state, :])\n",
        "\n",
        "    state, reward, done, _ = env.step(best_action)\n",
        "\n",
        "    cumulative_reward += reward\n",
        "    time.sleep(0.5)\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    print('Episode Reward :', cumulative_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZflvmE4QY4Y",
        "outputId": "e0816e9d-4ce6-4390-e86f-afe755bcc1a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode Reward : -200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros((state_size, action_size))\n",
        "episodes = 100000\n",
        "learning_rate = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "\n",
        "def greedy_policy(state, table):\n",
        "    z = np.random.random()\n",
        "    if z > epsilon:\n",
        "        action = np.argmax(table[state])\n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "    return action"
      ],
      "metadata": {
        "id": "B0Td5pLpQf8v"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "GC5gaJSsRizA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_table = np.zeros((state_size, action_size))\n",
        "episodes = 100000\n",
        "deltas = []\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done =False\n",
        "    step = 0\n",
        "    change_t = 0\n",
        "    if episode % 5000 == 0:\n",
        "        print(\"Episode: {}\".format(episode))\n",
        "\n",
        "    while not done:\n",
        "        action = greedy_policy(state, s_table)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_action = greedy_policy(state, s_table)\n",
        "        old_t = s_table[state, action]\n",
        "\n",
        "        s_table[state, action] += learning_rate * (reward+gamma* s_table[next_state, next_action] - s_table[state, action])\n",
        "        change_t = max(change_t, np.abs(s_table[state][action] - old_t))\n",
        "\n",
        "        state = next_state\n",
        "    deltas.append(change_t)\n",
        "    if deltas[-1] < 0.000000001:\n",
        "        break\n",
        "    episode += 1\n",
        "print(\"Maximum Difference is :\", deltas[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj2F9z6FRlGu",
        "outputId": "eee12e70-9099-4dcb-dc9c-7850a929ea8c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 5000\n",
            "Episode: 10000\n",
            "Episode: 15000\n",
            "Episode: 20000\n",
            "Episode: 25000\n",
            "Episode: 30000\n",
            "Episode: 35000\n",
            "Episode: 40000\n",
            "Episode: 45000\n",
            "Episode: 50000\n",
            "Episode: 55000\n",
            "Episode: 60000\n",
            "Episode: 65000\n",
            "Episode: 70000\n",
            "Episode: 75000\n",
            "Episode: 80000\n",
            "Episode: 85000\n",
            "Episode: 90000\n",
            "Episode: 95000\n",
            "Episode: 100000\n",
            "Maximum Difference is : 0.9196494312444727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "metadata": {
        "id": "-HDysM8ORnmF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while(done==False):\n",
        "    best_action = np.argmax(s_table[state, :])\n",
        "\n",
        "    state, reward, done, _ = env.step(best_action)\n",
        "\n",
        "    cumulative_reward += reward\n",
        "    time.sleep(0.5)\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    print('Episode Reward :', cumulative_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmksBccZS9eK",
        "outputId": "07f6eaaa-8666-4752-df0a-140116bdd098"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode Reward : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while(done==False):\n",
        "    best_action = np.argmax(s_table[state, :])\n",
        "\n",
        "    state, reward, done, _ = env.step(best_action)\n",
        "\n",
        "    cumulative_reward += reward\n",
        "    time.sleep(0.5)\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    print('Episode Reward :', cumulative_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r67ayqWiS_-U",
        "outputId": "a0377cb1-f08d-4c38-f3cd-988c791b4d8a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode Reward : -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5kqxNi2TCX6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}